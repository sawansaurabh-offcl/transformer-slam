{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlthmUQ3EMxq"
      },
      "outputs": [],
      "source": [
        "pip install -q transformers torch torchvision pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(MODEL_NAME) # resizing and normalization\n",
        "model = AutoModelForImageClassification.from_pretrained(MODEL_NAME) # pulls the model\n",
        "\n",
        "model.eval() # because we are using pre-trained models and doing only inference\n"
      ],
      "metadata": {
        "id": "gDyCrOWCEz6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting a sample image\n",
        "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"\n",
        "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
        "\n",
        "# processing it for Vit with our processor\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "pred_id = logits.argmax(dim=-1).item() # takes largest output logit (predicted class ID) and converts to python int from tensor\n",
        "label = model.config.id2label[pred_id] # gets the label for the predicted class ID from internal look-up dictionary stored in HF model.\n",
        "\n",
        "print(\"Prediction:\", label)\n"
      ],
      "metadata": {
        "id": "GRq-nhQ2E3Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "_wPHGtizUQt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing attention maps\n",
        "model.set_attn_implementation(\"eager\") # to ensure attention tensor is stored in memory for extraction\n",
        "\n",
        "outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
        "\n",
        "attentions = outputs.attentions      # attention maps. tuple: [layers, B, heads, tokens, tokens]\n",
        "hidden_states = outputs.hidden_states # feature maps. tuple: [layers, B, tokens, dim]\n"
      ],
      "metadata": {
        "id": "6BNR_PbBE72B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn = attentions[-1]        # [B, heads, N, N]\n",
        "attn = attn.mean(dim=1)     # [B, N, N]\n",
        "cls_attn = attn[:, 0, 1:]   # [B, num_patches]\n",
        "cls_attn = cls_attn / cls_attn.max()\n",
        "\n",
        "heatmap = cls_attn.reshape(14, 14)\n",
        "# Detach the tensor and convert to numpy before plotting\n",
        "plt.imshow(heatmap.detach().numpy(), cmap=\"jet\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "ow5Wj5gnXffl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting attention rollout\n",
        "\n",
        "\"\"\"\n",
        "How it works conceptually :\n",
        "\n",
        "For each layer:\n",
        "\n",
        "- Average attention heads\n",
        "- Add identity matrix (residual connection)\n",
        "- Normalize rows\n",
        "- Multiply attention matrices layer by layer\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "Rollout = A₁ · A₂ · A₃ · ... · Aₙ\n",
        "\n",
        "Where each Aᵢ is a layer’s attention matrix.\n",
        "\"\"\"\n",
        "def attention_rollout(attentions):\n",
        "    rollout = torch.eye(attentions[0].size(-1)).to(attentions[0].device)\n",
        "\n",
        "    for attn in attentions:\n",
        "        attn = attn.mean(dim=1)           # avg heads\n",
        "        attn = attn + torch.eye(attn.size(-1)).to(attn.device)\n",
        "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
        "        rollout = attn @ rollout\n",
        "\n",
        "    return rollout\n",
        "\n",
        "rollout = attention_rollout(attentions)\n",
        "cls_rollout = rollout[0, 0, 1:]   # CLS → patches\n",
        "cls_rollout = cls_rollout / cls_rollout.max()\n",
        "\n",
        "heatmap = cls_rollout.reshape(14, 14)\n",
        "plt.imshow(heatmap.detach().numpy(), cmap=\"jet\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "lIN2IaszZO9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlay_heatmap_on_image(image, heatmap, alpha=0.5, cmap=\"jet\"):\n",
        "    \"\"\"\n",
        "    image  : PIL.Image | numpy array | torch tensor  (H,W,3)\n",
        "    heatmap: 2D tensor/array (H,W) or (h,w) → will be resized\n",
        "    \"\"\"\n",
        "\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import cv2\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # ---- Convert image to numpy ----\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.detach().cpu().numpy()\n",
        "        if image.shape[0] == 3:  # CHW → HWC\n",
        "            image = image.transpose(1, 2, 0)\n",
        "\n",
        "    if hasattr(image, \"convert\"):  # PIL\n",
        "        image = np.array(image)\n",
        "\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    # ---- Convert heatmap to numpy ----\n",
        "    if isinstance(heatmap, torch.Tensor):\n",
        "        heatmap = heatmap.detach().cpu().numpy()\n",
        "\n",
        "    # ---- Resize heatmap if needed ----\n",
        "    if heatmap.shape != (h, w):\n",
        "        heatmap = cv2.resize(heatmap, (w, h))\n",
        "\n",
        "    # ---- Normalize heatmap ----\n",
        "    heatmap = heatmap - heatmap.min()\n",
        "    heatmap = heatmap / (heatmap.max() + 1e-8)\n",
        "\n",
        "    # ---- Visualize ----\n",
        "    plt.imshow(image)\n",
        "    plt.imshow(heatmap, cmap=cmap, alpha=alpha)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "overlay_heatmap_on_image(image, heatmap)\n"
      ],
      "metadata": {
        "id": "63IyZKRzay4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for fine-tuning. Ex: setting model optimizer, new learning rate\n",
        "\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# we take output logits (same as in block 4 after a forward pass)\n",
        "logits = outputs.logits\n",
        "\n",
        "# we specify our G.T in tensor format\n",
        "target_labels = torch.tensor([0], device=logits.device) # Ensure device matches logits\n",
        "\n",
        "# Calculate a loss using CrossEntropyLoss\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "loss = loss_fn(logits, target_labels)\n",
        "\n",
        "# backpropagate\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "NSz_9g3ZE8jo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}